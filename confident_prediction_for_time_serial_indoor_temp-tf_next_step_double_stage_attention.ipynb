{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confident_prediction_for_time_serial_indoor_temp - double_stage_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import optparse\n",
    "import configparser\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "#import holidays\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_space = pd.read_excel('./datasets/88688_gap_15min_space_temp_f18_csw.xlsx')\n",
    "temp_weather = pd.read_excel('./datasets/68364_gap_1h_weather_temp.xlsx')\n",
    "\n",
    "df_weather=pd.DataFrame(np.array(temp_weather),columns=['Run_DateTime','Date','UTC_Date','TempA','TempM',\n",
    "                                                        'DewPointA','DewPointM' ,'Humidity' ,'WindSpeedA',\n",
    "                                                        'WindSpeedM' ,'WindGustA' ,'WindGustM' ,'WindDir',\n",
    "                                                        'VisibilityA' ,'VisibilityM' ,'PressureA' ,'PressureM',\n",
    "                                                        'WindChillA' ,'WindChillM' ,'HeatIndexA' ,'HeatIndexM',\n",
    "                                                        'PrecipA' ,'PrecipM' ,'Condition' ,'Fog' ,'Rain',\n",
    "                                                        'Snow' ,'Hail' ,'Thunder','Tornado' ,'ID'])\n",
    "df_weather = df_weather[['Date','TempM','DewPointM','Humidity','WindSpeedM' ,'PressureM']]\n",
    "\n",
    "df_space = pd.DataFrame(np.array(temp_space),columns=['ID','zone','floor','quadrant','eq_no','Date','temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_process(date):\n",
    "    time=datetime.datetime.strptime(date,'%Y-%m-%d %H:%M:%S.0000000')\n",
    "    return datetime.datetime(time.year,time.month,time.day,time.hour)\n",
    "\n",
    "def weather_data_feature_selected_and_serialize(df_weather):\n",
    "    cols=['TempM','DewPointM','Humidity','WindSpeedM' ,'PressureM']\n",
    "    for col in cols:\n",
    "        df_weather = df_weather.loc[~(df_weather[col] == 'N/A')]\n",
    "        df_weather = df_weather.loc[~(df_weather[col] == -9999)]\n",
    "    date_trans=lambda date:datetime.datetime(date.year, date.month, date.day, date.hour)\n",
    "    df_weather['time']=df_weather['Date'].apply(date_trans)\n",
    "    df_weather = df_weather.dropna()\n",
    "    df_weather.drop('Date',axis=1,inplace=True)\n",
    "    result = df_weather.groupby('time').apply(np.mean)\n",
    "    return result\n",
    "\n",
    "def weather_data_interpolation(df_weather):\n",
    "    interpolate_sample = df_weather.resample('15Min').asfreq()\n",
    "    df_weather_interpolated = interpolate_sample.interpolate(method='time')\n",
    "    return df_weather_interpolated\n",
    "\n",
    "def space_temp_data_feature_selected_and_serialize(df_space):\n",
    "    cols=['Date','temp']\n",
    "    df_space = df_space[cols]\n",
    "    df_space = df_space.dropna()\n",
    "    date_trans=lambda date:datetime.datetime(date.year, date.month, date.day, date.hour, date.minute)\n",
    "    df_space['time'] = df_space['Date'].apply(date_trans)\n",
    "    df_space.drop('Date', axis=1, inplace=True)\n",
    "    result = df_space.groupby('time').apply(np.mean)\n",
    "    return result\n",
    "\n",
    "def space_temp_extreme_data_clean_up(df_space, bad_values, verbose):\n",
    "    df_space_cleaned = df_space.copy()\n",
    "    \n",
    "    clean_up_temp = []\n",
    "    \n",
    "    for bad_value in bad_values:\n",
    "        for i in df_space.loc[df_space['temp'] == bad_value].index:\n",
    "            clean_up_temp.append((df_space_cleaned.loc[i]))\n",
    "            df_space_cleaned = df_space_cleaned.drop(i)\n",
    "    \n",
    "    if(verbose == True):\n",
    "        show_interpolation_data_range(clean_up_temp)\n",
    "    \n",
    "    return df_space_cleaned\n",
    "\n",
    "def space_temp_data_interpolation(df_space):\n",
    "    interpolate_sample = df_space.resample('15Min').asfreq()\n",
    "    df_space_interpolated = interpolate_sample.interpolate(method='time')\n",
    "    return df_space_interpolated\n",
    "\n",
    "def weather_data_preprocess(df_weather, bad_values=[0, 162.8, 250]):\n",
    "    df_weather_serial = weather_data_feature_selected_and_serialize(df_weather)\n",
    "    df_weather_serial_interpolated = weather_data_interpolation(df_weather_serial)    \n",
    "    return df_weather_serial_interpolated\n",
    "    \n",
    "def space_temp_data_preprocess(df_space, bad_values=[0, 162.8, 250], verbose=False):\n",
    "    df_space_serial = space_temp_data_feature_selected_and_serialize(df_space)\n",
    "    df_space_serial_cleaned = space_temp_extreme_data_clean_up(df_space_serial, bad_values, verbose)\n",
    "    df_space_serial_interpolated = space_temp_data_interpolation(df_space_serial_cleaned)\n",
    "    \n",
    "    return df_space_serial_interpolated\n",
    "\n",
    "# In case people want to check the data after preprocessing\n",
    "def write_data_to_excel(dataframe, filename):\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "\n",
    "    # Convert the dataframe to an XlsxWriter Excel object.\n",
    "    dataframe.to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "    # Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.save()\n",
    "    \n",
    "# cleanning data printer\n",
    "def show_interpolation_data_range(clean_up_list):\n",
    "    bad_data_counter = {}\n",
    "    \n",
    "    for i, row in enumerate(clean_up_list):\n",
    "        print(  'Temp: %s   '%(row['temp']), 'Date:', row.name)\n",
    "        if(str(row['temp']) not in bad_data_counter):\n",
    "            bad_data_counter[str(row['temp'])] = 1\n",
    "        else:\n",
    "            bad_data_counter[str(row['temp'])] += 1\n",
    "    \n",
    "    # Print counter\n",
    "    for key, value in bad_data_counter.items():\n",
    "        print('Clean up %s bad data(%s)' %(value, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp: 0.0    Date: 2012-06-21 07:30:00\n",
      "Temp: 0.0    Date: 2012-06-21 07:45:00\n",
      "Temp: 0.0    Date: 2012-06-21 08:00:00\n",
      "Temp: 0.0    Date: 2012-06-21 08:15:00\n",
      "Temp: 0.0    Date: 2012-06-21 08:30:00\n",
      "Temp: 0.0    Date: 2012-06-21 08:45:00\n",
      "Temp: 0.0    Date: 2013-02-04 08:30:00\n",
      "Temp: 0.0    Date: 2013-02-04 08:45:00\n",
      "Temp: 0.0    Date: 2013-02-04 09:00:00\n",
      "Temp: 0.0    Date: 2013-02-04 09:30:00\n",
      "Temp: 0.0    Date: 2014-05-01 17:15:00\n",
      "Temp: 0.0    Date: 2014-05-13 14:45:00\n",
      "Temp: 0.0    Date: 2014-07-11 10:15:00\n",
      "Temp: 162.8    Date: 2014-03-13 05:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 10:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 10:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 10:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 11:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 11:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 11:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 11:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 12:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 12:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 12:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 12:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 13:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 13:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 13:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 13:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 14:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 14:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 14:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 14:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 15:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 15:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 15:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 15:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 16:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 16:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 16:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 16:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 17:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 17:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 17:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 17:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 18:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 18:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 18:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 18:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 19:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 19:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 19:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 19:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 20:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 20:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 20:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 20:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 21:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 21:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 21:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 21:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 22:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 22:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 22:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 22:45:00\n",
      "Temp: 250.0    Date: 2014-03-12 23:00:00\n",
      "Temp: 250.0    Date: 2014-03-12 23:15:00\n",
      "Temp: 250.0    Date: 2014-03-12 23:30:00\n",
      "Temp: 250.0    Date: 2014-03-12 23:45:00\n",
      "Temp: 250.0    Date: 2014-03-13 00:00:00\n",
      "Temp: 250.0    Date: 2014-03-13 00:15:00\n",
      "Temp: 250.0    Date: 2014-03-13 00:30:00\n",
      "Temp: 250.0    Date: 2014-03-13 00:45:00\n",
      "Temp: 250.0    Date: 2014-03-13 01:00:00\n",
      "Temp: 250.0    Date: 2014-03-13 01:15:00\n",
      "Temp: 250.0    Date: 2014-03-13 01:30:00\n",
      "Temp: 250.0    Date: 2014-03-13 01:45:00\n",
      "Temp: 250.0    Date: 2014-03-13 02:00:00\n",
      "Temp: 250.0    Date: 2014-03-13 02:15:00\n",
      "Temp: 250.0    Date: 2014-03-13 02:30:00\n",
      "Temp: 250.0    Date: 2014-03-13 02:45:00\n",
      "Temp: 250.0    Date: 2014-03-13 03:00:00\n",
      "Temp: 250.0    Date: 2014-03-13 03:15:00\n",
      "Temp: 250.0    Date: 2014-03-13 03:30:00\n",
      "Temp: 250.0    Date: 2014-03-13 03:45:00\n",
      "Temp: 250.0    Date: 2014-03-13 04:00:00\n",
      "Temp: 250.0    Date: 2014-03-13 04:15:00\n",
      "Temp: 250.0    Date: 2014-03-13 04:30:00\n",
      "Temp: 250.0    Date: 2014-03-13 04:45:00\n",
      "Temp: 250.0    Date: 2014-03-13 05:00:00\n",
      "Clean up 13 bad data(0.0)\n",
      "Clean up 1 bad data(162.8)\n",
      "Clean up 76 bad data(250.0)\n"
     ]
    }
   ],
   "source": [
    "df_weather_processed = weather_data_preprocess(df_weather)\n",
    "df_space_processed = space_temp_data_preprocess(df_space, bad_values=[0, 162.8, 250], verbose=True)\n",
    "#write_data_to_excel(df_space_processed, 'space_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_weather_processed, df_space_processed, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TempM</th>\n",
       "      <th>DewPointM</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>WindSpeedM</th>\n",
       "      <th>PressureM</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>93199.000000</td>\n",
       "      <td>93199.000000</td>\n",
       "      <td>93199.000000</td>\n",
       "      <td>93199.000000</td>\n",
       "      <td>93199.000000</td>\n",
       "      <td>93199.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.980013</td>\n",
       "      <td>5.007351</td>\n",
       "      <td>61.041433</td>\n",
       "      <td>8.613126</td>\n",
       "      <td>1016.870510</td>\n",
       "      <td>75.989889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.102947</td>\n",
       "      <td>10.978114</td>\n",
       "      <td>18.255298</td>\n",
       "      <td>5.335288</td>\n",
       "      <td>7.650253</td>\n",
       "      <td>1.991362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-15.600000</td>\n",
       "      <td>-26.700000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>966.566667</td>\n",
       "      <td>67.802124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.400000</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>1011.883333</td>\n",
       "      <td>74.566101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.900000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>8.325000</td>\n",
       "      <td>1016.850000</td>\n",
       "      <td>75.586365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.700000</td>\n",
       "      <td>14.150000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>1021.764583</td>\n",
       "      <td>76.946655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36.700000</td>\n",
       "      <td>24.233333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>42.566667</td>\n",
       "      <td>1041.600000</td>\n",
       "      <td>87.527222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TempM     DewPointM      Humidity    WindSpeedM     PressureM  \\\n",
       "count  93199.000000  93199.000000  93199.000000  93199.000000  93199.000000   \n",
       "mean      12.980013      5.007351     61.041433      8.613126   1016.870510   \n",
       "std       10.102947     10.978114     18.255298      5.335288      7.650253   \n",
       "min      -15.600000    -26.700000     13.000000      0.000000    966.566667   \n",
       "25%        4.400000     -3.600000     47.000000      5.600000   1011.883333   \n",
       "50%       13.900000      6.100000     59.500000      8.325000   1016.850000   \n",
       "75%       21.700000     14.150000     75.000000     11.150000   1021.764583   \n",
       "max       36.700000     24.233333    100.000000     42.566667   1041.600000   \n",
       "\n",
       "               temp  \n",
       "count  93199.000000  \n",
       "mean      75.989889  \n",
       "std        1.991362  \n",
       "min       67.802124  \n",
       "25%       74.566101  \n",
       "50%       75.586365  \n",
       "75%       76.946655  \n",
       "max       87.527222  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0,1))\n",
    "def get_batch(df, batch_size=128, T=16, input_dim=6, step=0, train=True):\n",
    "    \n",
    "    t = step * batch_size\n",
    "    X_batch = np.empty(shape=[batch_size, T, input_dim])\n",
    "    y_batch = np.empty(shape=[batch_size, T])\n",
    "    labels = np.empty(shape=[batch_size])\n",
    "    #time_batch = np.empty(shape=[batch_size, T], dtype='datetime64')\n",
    "    time_batch = np.array([])\n",
    "    time_stamp = df.index.tolist()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        X_batch[i, :] = scaler.fit_transform(df.iloc[t:t+T].values)\n",
    "        y_batch[i, :] = df[\"temp\"].iloc[t:t+T].values\n",
    "        labels[i] = df[\"temp\"].iloc[t+T]\n",
    "        time_batch = np.append(time_batch, time_stamp[t+T])\n",
    "        #time_batch[i, :] = df.iloc[t:t+T].index[-1]\n",
    "        \n",
    "        t += 1     \n",
    "    \n",
    "    ## shuffle in train, not in test\n",
    "    if train:\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "        X_batch = X_batch[index]\n",
    "        y_batch = y_batch[index]\n",
    "        labels = labels[index]\n",
    "        time_batch = time_batch[index]\n",
    "\n",
    "    return X_batch, y_batch, labels, time_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, labels, time_train = get_batch(df,batch_size = 256, T = 7*24, step = 50, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 168, 6)\n",
      "(256, 168)\n",
      "(256,)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(labels.shape)\n",
    "print(time_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ts_prediction(object):\n",
    "    \n",
    "    def __init__(self, input_dim, time_step, n_hidden, d_hidden, batch_size):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.d_hidden = d_hidden\n",
    "        #self.o_hidden = 16\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.time_step = time_step\n",
    "        \n",
    "        self.seq_len = tf.placeholder(tf.int32,[None])\n",
    "        self.input_x = tf.placeholder(dtype = tf.float32, shape = [None, None, input_dim]) # b,T,d_in\n",
    "        self.input_y = tf.placeholder(dtype = tf.float32,shape = [None,self.time_step]) # b,T\n",
    "        self.label = tf.placeholder(dtype = tf.float32) #b,1\n",
    "        self.original_loss = tf.placeholder(dtype = tf.float32, shape = [])\n",
    "        ## placeholder for loss without adversarial gradient added\n",
    "        \n",
    "        self.encode_cell = tf.contrib.rnn.LSTMCell(self.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        self.decode_cell = tf.contrib.rnn.LSTMCell(self.d_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        #self.output_cell = tf.contrib.rnn.LSTMCell(self.o_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        \n",
    "        self.loss_1 = tf.constant(0.0)\n",
    "        self.loss = tf.constant(0.0)\n",
    "        ## ===========  build the model =========== ##\n",
    "            \n",
    "        ## ==== encoder ===== ## \n",
    "        h_encode, c_state = self.en_RNN(self.input_x)#!!!!!!!\n",
    "        c_expand = tf.tile(tf.expand_dims(c_state[1],1),[1,self.time_step,1])\n",
    "        fw_lstm = tf.concat([h_encode,c_expand],axis = 2) # b,T,2n\n",
    "        stddev = 1.0/(self.n_hidden*self.time_step)\n",
    "        Ue = tf.get_variable(name= 'Ue',dtype = tf.float32,\n",
    "                             initializer = tf.truncated_normal(mean = 0.0, stddev = stddev,shape = [self.time_step,self.time_step]))\n",
    "        ## (b,d,T) * (b,T,T)  = (b,d,T)\n",
    "        brcast_UX = tf.matmul(tf.transpose(self.input_x,[0,2,1]),tf.tile(tf.expand_dims(Ue,0),[self.batch_size,1,1]))\n",
    "        e_list = []\n",
    "        for k in range(self.input_dim):\n",
    "            feature_k = brcast_UX[:,k,:] \n",
    "            e_k = self.en_attention(fw_lstm,feature_k) # b,T \n",
    "            e_list.append(e_k)\n",
    "        e_mat = tf.concat(e_list,axis = 2)\n",
    "        alpha_mat = tf.nn.softmax(e_mat) #b,T,d_in horizontally\n",
    "        encode_input = tf.multiply(self.input_x,alpha_mat)\n",
    "        self.h_t, self.c_t = self.en_RNN(encode_input, scopes = 'fw_lstm')\n",
    "           \n",
    "        ## ==== inferrence nn ==== ##\n",
    "        \"\"\"\n",
    "        node_hidden_layer1 = 500\n",
    "        node_hidden_layer2 = 500\n",
    "        node_hidden_layer3 = 500\n",
    "        hidden_layer1 = {\"weights\": tf.Variable(tf.random_normal([10, node_hidden_layer1])),\n",
    "                          \"biases\": tf.Variable(tf.random_normal([node_hidden_layer1]))}\n",
    "\n",
    "        hidden_layer2 = {\"weights\": tf.Variable(tf.random_normal([node_hidden_layer1, node_hidden_layer2])),\n",
    "                          \"biases\": tf.Variable(tf.random_normal([node_hidden_layer2]))}\n",
    "\n",
    "        hidden_layer3 = {\"weights\": tf.Variable(tf.random_normal([node_hidden_layer2, node_hidden_layer3])),\n",
    "                          \"biases\": tf.Variable(tf.random_normal([node_hidden_layer3]))}\n",
    "\n",
    "        output_layer = {\"weights\": tf.Variable(tf.random_normal([node_hidden_layer3, 20])),\n",
    "                        \"biases\": tf.Variable(tf.random_normal([20]))}\n",
    "\n",
    "        # Hidden Layer: weights \\dot input_data + biaes\n",
    " \n",
    "        layer1 = tf.add(tf.matmul(self.h_t, hidden_layer1[\"weights\"]), hidden_layer1[\"biases\"])\n",
    "        layer1 = tf.nn.relu(layer1)\n",
    "\n",
    "        layer2 = tf.add(tf.matmul(layer1, hidden_layer2[\"weights\"]), hidden_layer2[\"biases\"])\n",
    "        layer2 = tf.nn.relu(layer2)\n",
    "\n",
    "        layer3 = tf.add(tf.matmul(layer2, hidden_layer3[\"weights\"]), hidden_layer3[\"biases\"])\n",
    "        layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "        output = tf.add(tf.matmul(layer3, output_layer[\"weights\"]), output_layer[\"biases\"])\n",
    "        \n",
    "        prediction = neural_network_model(X_data)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \"\"\"\n",
    "            \n",
    "        ## ==== decoder ===== ## \n",
    "        h_decode, d_state = self.de_RNN(tf.expand_dims(self.input_y,-1))\n",
    "        \n",
    "        d_expand = tf.tile(tf.expand_dims(d_state[1],1),[1,self.time_step,1])\n",
    "        dec_lstm = tf.concat([h_decode,d_expand],axis = 2) # b,T,2*d_hidden\n",
    "\n",
    "        Ud = tf.get_variable(name = 'Ud', dtype = tf.float32,\n",
    "                             initializer = tf.truncated_normal(mean = 0.0, stddev = stddev, shape = [self.n_hidden, self.n_hidden]))\n",
    "        \n",
    "        brcast_UDX = tf.matmul(self.h_t,tf.tile(tf.expand_dims(Ud,0),[self.batch_size,1,1])) # b,T,n_hidden\n",
    "        \n",
    "        l_list = []\n",
    "        for i in range(self.time_step):\n",
    "            feature_i = brcast_UDX[:,i,:]\n",
    "            l_i = self.dec_attention(dec_lstm,feature_i)\n",
    "            l_list.append(l_i)\n",
    "        l_mat = tf.concat(l_list,axis = 2)\n",
    "        beta_mat = tf.nn.softmax(l_mat, dim = 1)\n",
    "        context_list = []\n",
    "        h_tmp = tf.transpose(self.h_t,[0,2,1])\n",
    "\n",
    "        for t in range(self.time_step):\n",
    "            beta_t = tf.reshape(beta_mat[:,t,:],[self.batch_size,1,self.time_step])\n",
    "            self.c_t = tf.reduce_sum(tf.multiply(h_tmp,beta_t),2) # b,T,T -> b,T,1\n",
    "            context_list.append(self.c_t)\n",
    "        c_context = tf.stack(context_list,axis = 2) # b,n_hidden,T\n",
    "        # b,T,1 b,T,n_hidden -> b,T,n_hidden+1\n",
    "        c_concat = tf.concat([tf.expand_dims(self.input_y,-1),tf.transpose(c_context,[0,2,1])], axis = 2)\n",
    "        W_hat = tf.get_variable(name = 'W_hat', dtype = tf.float32,\n",
    "                                initializer = tf.truncated_normal(mean = 0.0, stddev = stddev,shape = [self.n_hidden+1,1]))\n",
    "        y_encode = tf.matmul(c_concat,tf.tile(tf.expand_dims(W_hat,0),[self.batch_size,1,1]))\n",
    "        \n",
    "        \n",
    "        h_out, d_out = self.de_RNN(y_encode)\n",
    "        \n",
    "        last_concat = tf.expand_dims(tf.concat([h_out[:,-1,:],d_out[-1]],axis = 1),1)\n",
    "        Wy = tf.get_variable(name = 'Wy', dtype = tf.float32,initializer = tf.truncated_normal(mean = 0.0, stddev = stddev,shape = [self.n_hidden+self.d_hidden,1]))\n",
    "        W_y = tf.tile(tf.expand_dims(Wy,0),[self.batch_size,1,1])\n",
    "        self.y_predict = tf.squeeze(tf.matmul(last_concat,W_y))\n",
    "        #self.loss += tf.reduce_mean(tf.square(self.label - self.y_predict)) # reduce_mean: avg of batch loss\n",
    "        \n",
    "        self.loss_1 += tf.reduce_mean(tf.square(self.label - self.y_predict)) # reduce_mean: avg of batch loss\n",
    "        self.adversarial_gradient = tf.gradients(self.loss_1,self.input_x)\n",
    "        \n",
    "        self.loss = self.loss_1 + self.original_loss\n",
    "        \n",
    "        self.params = tf.trainable_variables()\n",
    "        \n",
    "        #learning rate\n",
    "        self.alpha=tf.Variable(5e-4)\n",
    "        optimizer = tf.train.AdamOptimizer(self.alpha)\n",
    "        \n",
    "        \n",
    "        #self.train_op = optimizer.minimize(self.loss)\n",
    "        grad_var = optimizer.compute_gradients(loss = self.loss, var_list = self.params, aggregation_method = 2)\n",
    "        self.train_op = optimizer.apply_gradients(grad_var)\n",
    "        \n",
    "        \n",
    "    def en_RNN(self, input_x, scopes = 'fw_lstm'):\n",
    "        '''\n",
    "        input_x: b, T, d_in\n",
    "        \n",
    "        output: h:       seqence of output state b,T,n_hidden\n",
    "                state:   final state b,n_hidden\n",
    "                \n",
    "        '''\n",
    "        with tf.variable_scope('fw_lstm' or scopes) as scope:\n",
    "            try:\n",
    "                h,state = tf.nn.dynamic_rnn(\n",
    "                    cell = self.encode_cell, inputs = input_x,\n",
    "                    sequence_length = self.seq_len,\n",
    "                    dtype = tf.float32, scope = 'fw_lstm')\n",
    "                \n",
    "            except ValueError:\n",
    "                scope.reuse_variables()\n",
    "                h,state = tf.nn.dynamic_rnn(\n",
    "                    cell = self.encode_cell, inputs = input_x,\n",
    "                    sequence_length = self.seq_len,\n",
    "                    dtype = tf.float32, scope = scopes)\n",
    "        \n",
    "        return [h,state]\n",
    "    \n",
    "    def de_RNN(self,input_y, scopes = 'de_lstm'):\n",
    "        \n",
    "        with tf.variable_scope('dec_lstm') as scope:\n",
    "            try:\n",
    "                h,state = tf.nn.dynamic_rnn(\n",
    "                    cell = self.decode_cell, inputs = input_y,\n",
    "                    sequence_length = self.seq_len,\n",
    "                    dtype = tf.float32, scope = 'de_lstm')\n",
    "                \n",
    "            except ValueError:\n",
    "                scope.reuse_variables()\n",
    "                h,state = tf.nn.dynamic_rnn(\n",
    "                    cell = self.decode_cell, inputs = input_y,\n",
    "                    sequence_length = self.seq_len,\n",
    "                    dtype = tf.float32, scope = scopes)\n",
    "        \n",
    "        return [h,state]\n",
    "    \n",
    "    def en_attention(self,fw_lstm,feature_k):\n",
    "        '''\n",
    "        fw_lstm: b,T,2n\n",
    "        feature_k: row k from brcast_UX, b,T\n",
    "        \n",
    "        return: b,T\n",
    "        '''\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            try:\n",
    "                mean = 0.0\n",
    "                stddev = 1.0/(self.n_hidden*self.time_step)\n",
    "                We = tf.get_variable(name = 'We', dtype=tf.float32,shape = [self.time_step, 2*self.n_hidden],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean,stddev))\n",
    "                Ve = tf.get_variable(name = 'Ve',dtype=tf.float32,shape = [self.time_step,1],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean,stddev))\n",
    "            except ValueError:\n",
    "                scope.reuse_variables()\n",
    "                We = tf.get_variable('We')\n",
    "                Ve = tf.get_variable('Ve')   \n",
    "            # (b,T,2n) (b,2n,T)\n",
    "        W_e = tf.transpose(tf.tile(tf.expand_dims(We,0),[self.batch_size,1,1]),[0,2,1]) # b,2n,T\n",
    "        mlp = tf.nn.tanh(tf.matmul(fw_lstm,W_e) + tf.reshape(feature_k,[self.batch_size,1,self.time_step])) #b,T,T + b,1,T = b,T,T\n",
    "        V_e = tf.tile(tf.expand_dims(Ve,0),[self.batch_size,1,1])\n",
    "        return  tf.matmul(mlp,V_e)\n",
    "            \n",
    "    def dec_attention(self, dec_lstm, feature_t, scopes = None):\n",
    "        '''\n",
    "        dec_lstm: b,T,2*d_hidden\n",
    "        feature_k: row k from brcast_UX, b,T\n",
    "        \n",
    "        return: b,T\n",
    "        '''\n",
    "        with tf.variable_scope('decoder' or scopes) as scope:\n",
    "            try:\n",
    "                mean = 0.0\n",
    "                stddev = 1.0/(self.n_hidden*self.time_step)\n",
    "                Wd = tf.get_variable(name = 'Wd', dtype=tf.float32, shape = [self.n_hidden, 2*self.d_hidden],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean,stddev))\n",
    "                Vd = tf.get_variable(name = 'Vd', dtype=tf.float32, shape = [self.n_hidden,1],\n",
    "                                     initializer=tf.truncated_normal_initializer(mean,stddev))\n",
    "            except ValueError:\n",
    "                scope.reuse_variables()\n",
    "                Wd = tf.get_variable('Wd')\n",
    "                Vd = tf.get_variable('Vd')   \n",
    "        # (b,T,2*d_hidden) (b,2*d_hidden,T)\n",
    "        W_d = tf.transpose(tf.tile(tf.expand_dims(Wd,0),[self.batch_size,1,1]),[0,2,1]) # b,2*d_hidden,n_hidden\n",
    "        # (b,T,2*d_hidden) * (b,2*d_hidden,n_hidden) -> b,T,n_hidden\n",
    "        mlp = tf.nn.tanh(tf.matmul(dec_lstm,W_d) + tf.reshape(feature_t,[self.batch_size,1,self.n_hidden])) #b,T,n_hidden + b,1,n_hidden = b,T,n_hidden\n",
    "        V_d = tf.tile(tf.expand_dims(Vd,0),[self.batch_size,1,1])\n",
    "        return  tf.matmul(mlp,V_d) #b,T,1\n",
    "    \n",
    "    def predict(self,x_test,y_test,sess):\n",
    "        \n",
    "        train_seq_len =  np.ones(self.batch_size) * self.time_step\n",
    "        feed = {model.input_x: x_test, \n",
    "                model.seq_len: train_seq_len,\n",
    "                model.input_y: y_test}\n",
    "        y_hat = sess.run(self.y_predict,feed_dict = feed)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data 73920 testing data 19279\n"
     ]
    }
   ],
   "source": [
    "w_data=df\n",
    "batch_size = 7 * 24 * 4 * 2\n",
    "INPUT_DIM = 6 # input feature\n",
    "# time_steps = 7*24 \n",
    "time_steps = 24 \n",
    "n_hidden = 64 # encoder dim\n",
    "d_hidden = 64 # decoder dim\n",
    "\n",
    "total_epoch = 10\n",
    "\n",
    "train_batch_num = int(len(w_data)*0.8/batch_size) ## 0.8 of traininng data, 20% for testing\n",
    "\n",
    "df_test = w_data[(train_batch_num*batch_size):]\n",
    "df1 = w_data[:(train_batch_num*batch_size)]\n",
    "\n",
    "steps_train = int((len(df1) - time_steps)/batch_size) - 1\n",
    "steps_test = int((len(df_test) - time_steps)/batch_size) - 1\n",
    "\n",
    "print ('Training data %i' % len(df1), \"testing data %i\" % len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "13\n",
      "(93199, 6)\n",
      "(19279, 6)\n",
      "(73920, 6)\n",
      "(1344, 24, 6)\n",
      "(1344, 24)\n",
      "(1344,)\n",
      "(1344,)\n"
     ]
    }
   ],
   "source": [
    "# Check everything before training model\n",
    "print(steps_train)\n",
    "print(steps_test)\n",
    "\n",
    "print(w_data.shape)\n",
    "print(df_test.shape)\n",
    "print(df1.shape)\n",
    "\n",
    "x, y, labels, times = get_batch(df1,batch_size = batch_size, T = time_steps, step = steps_train-1)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(labels.shape)\n",
    "print(times.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:103.4 epoch:0/10 loss:588382.248047\n"
     ]
    }
   ],
   "source": [
    "current_epoch = 0\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = ts_prediction(input_dim=INPUT_DIM, time_step=time_steps, n_hidden=n_hidden, d_hidden=d_hidden, batch_size=batch_size)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#memory policy\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "while current_epoch <= total_epoch:\n",
    "    cumulative_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for t in range(steps_train - 1):\n",
    "        x, y, labels, _ = get_batch(df1,batch_size=batch_size, T=time_steps, step=t)\n",
    "        train_seq_len =  np.ones(batch_size) * time_steps\n",
    "        feed = {model.input_x: x, \n",
    "                model.seq_len: train_seq_len,\n",
    "                model.input_y: y,\n",
    "                model.label: labels,\n",
    "                }\n",
    "        loss_1, adversarial_gradients, _ = sess.run([model.loss_1,model.adversarial_gradient, model.alpha],feed_dict = feed)\n",
    "        \n",
    "        epsilon = np.random.uniform(0,0.5,1)\n",
    "        \n",
    "        feed = {model.input_x: x + epsilon*adversarial_gradients[0], \n",
    "                model.seq_len: train_seq_len,\n",
    "                model.input_y: y,\n",
    "                model.label: labels,\n",
    "                model.original_loss: loss_1,\n",
    "                }\n",
    "        loss, _, _ = sess.run([model.loss, model.train_op, model.alpha], feed_dict = feed)\n",
    "        \n",
    "        cumulative_loss += loss\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    print(\"time:%.1f epoch:%i/%i loss:%f\" % (end_time - start_time, current_epoch, total_epoch, cumulative_loss))\n",
    "    if cumulative_loss<3:\n",
    "        if cumulative_loss>2.5:\n",
    "            update=tf.assign(model.alpha,1e-4) #update the learning rate\n",
    "            sess.run(update)\n",
    "            print('Learning rate is updated to 1e-4')\n",
    "        else:\n",
    "            update=tf.assign(model.alpha,1e-5) #update the learning rate\n",
    "            sess.run(update)\n",
    "            print('Learning rate is updated to 1e-5')\n",
    "\n",
    "    current_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "y_hat_arr = np.empty(shape = [0])\n",
    "y_labels_arr = np.empty(shape = [0])\n",
    "time_batch = np.empty(shape = [0], dtype=\"str\")\n",
    "\n",
    "\n",
    "for t in range(steps_test-1):\n",
    "\n",
    "    x_test,y_test,labels_test, time_test = get_batch(df_test, batch_size=batch_size, T=time_steps, step=t, train=False)   \n",
    "    y_hat = model.predict(x_test, y_test, sess) \n",
    "    y_hat_arr = np.concatenate([y_hat_arr, np.array(y_hat)])\n",
    "    y_labels_arr = np.concatenate([y_labels_arr, np.array(labels_test)])\n",
    "    time_batch = np.concatenate([time_batch, time_test])\n",
    "    test_loss += np.mean(np.square(y_hat - labels_test))\n",
    "\n",
    "print (\"the mean squared error for test data are %f \" % (test_loss * 1.0 / steps_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_labels_arr.shape)\n",
    "print(y_hat_arr.shape)\n",
    "print(time_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Forecast 24 hours plot\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "%matplotlib inline\n",
    "\n",
    "_FUTURE_HOURS = y_hat_arr.shape[1]\n",
    "compare_data_index = [0, 100, 200, 300, 400, 500, 600, 700]\n",
    "\n",
    "X_SHOW_FREQUENT = 4\n",
    "Y_SHOW_FREQUENT = 24 * 7\n",
    "x_stick = list(range(0, y_labels_arr.shape[1] + 1, X_SHOW_FREQUENT))\n",
    "y_stick = list(range(0, y_labels_arr.shape[0] + 1, Y_SHOW_FREQUENT))\n",
    "time_batch[[y_stick_value for y_stick_value in y_stick]]\n",
    "x_stick_content = np.array([time_batch[0],'Pred 1 h', 'Pred 2 h', 'Pred 3 h', 'Pred 4 h',\n",
    "                           'Pred 5 h', 'Pred 6 h', 'Pred 7 h', 'Pred 8 h',\n",
    "                           'Pred 9 h', 'Pred 10 h', 'Pred 11 h', 'Pred 12 h',\n",
    "                           'Pred 13 h', 'Pred 14 h', 'Pred 15 h', 'Pred 16 h',\n",
    "                           'Pred 17 h', 'Pred 18 h', 'Pred 19 h', 'Pred 20 h',\n",
    "                           'Pred 21 h', 'Pred 22 h', 'Pred 23 h', 'Pred 24 h'])\n",
    "\n",
    "for index in compare_data_index:\n",
    "    x_stick_content[0] = time_batch[index]\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    plt.axis([0.0, _FUTURE_HOURS, 62.0, 82.0])\n",
    "    plt.gca().set_autoscale_on(False)\n",
    "    plt.plot(list(range(24)), y_hat_arr[index, :], 'blue', label=\"prediction label\")\n",
    "    plt.plot(list(range(24)), y_labels_arr[index, :], 'red', label=\"true label\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xticks(x_stick, x_stick_content[[x_stick_value for x_stick_value in x_stick]])\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plot_num = 24 * 7 * 4 * 2\n",
    "SHOW_FREQUENT = 12 * 4\n",
    "x_index = range(plot_num)\n",
    "x_stick = list(range(0, plot_num, SHOW_FREQUENT))\n",
    "\n",
    "for i in range(int(y_labels_arr.shape[0] / plot_num)):\n",
    "#for i in range(3):\n",
    "    start_idx = i * plot_num\n",
    "    end_idx = (i + 1) * plot_num\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    #plt.ylim([72.0, 82.0])\n",
    "    plt.xticks(x_stick, time_batch[[start_idx + x_stick_value for x_stick_value in x_stick]])\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.plot(range(plot_num), y_hat_arr[start_idx:end_idx], 'b', label=\"prediction\")\n",
    "    plt.plot(range(plot_num), y_labels_arr[start_idx:end_idx], 'r', label=\"ground true\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
